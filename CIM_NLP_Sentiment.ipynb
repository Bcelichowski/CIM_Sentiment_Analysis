{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1a2617327bae65c6a4d799e81fa840f9000ae74bb7b660fd632746a44d38d3e2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\BennettCelichowski\\AppData\\Roaming\\nltk_data.\n[nltk_data]     ..\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "# CREATE PDF OBJECTS AND BASIC PARAMS\n",
    "\n",
    "#pdf_obj = open('CIM-02-American-Casino.pdf','rb')\n",
    "pdf_Reader = PyPDF2.PdfFileReader(pdf_obj)\n",
    "\n",
    "num_pages = pdf_Reader.numPages\n",
    "\n",
    "print(num_pages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT TEXT VIA LOOP\n",
    "\n",
    "\n",
    "text_list = []\n",
    "\n",
    "for page in range(num_pages):\n",
    "    text = pdf_Reader.getPage(page).extractText()\n",
    "    text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CLEAN TEXT\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# LEMMATIZER\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# CLEANING LOOP\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "for word in range(num_pages):\n",
    "    #REMOVE PUNCTUATION\n",
    "    words = re.sub('[^a-zA-Z]',' ',text_list[word])\n",
    "    #MAKE EVERYHTING LOWERCASE\n",
    "    words=words.lower().split()\n",
    "    #ELIMINATE STOPWORDS\n",
    "    words =[lemmatizer.lemmatize(word) for word in words if (word not in stop_words)]\n",
    "    #CREATE LIST OF STOPWORDS\n",
    "    words=' '.join(words)\n",
    "    clean_data.append(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SEPARATE WORDS\n",
    "\n",
    "clean_words = []\n",
    "\n",
    "for page in clean_data:\n",
    "    for word in page.split():\n",
    "        clean_words.append(word)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(54, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "# SEPARATE WORDS & PAGE #'s\n",
    "\n",
    "\n",
    "def text_by_page(data):\n",
    "\n",
    "    clean_page = {}\n",
    "\n",
    "    for page in range(num_pages):\n",
    "        text = pdf_Reader.getPage(page).extractText()\n",
    "        pg = pdf_Reader.getPageNumber(pdf_Reader.getPage(page))\n",
    "        clean_page.update({pg:text})\n",
    "\n",
    "    clean_page = pd.DataFrame(clean_page, index=[0]).transpose()\n",
    "\n",
    "    return clean_page \n",
    "\n",
    "page_text = text_by_page(pdf_Reader)\n",
    "\n",
    "page_text.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(54, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "# CLEAN PAGE DATA\n",
    "\n",
    "clean_page_data = []\n",
    "\n",
    "def clean_pages(data):\n",
    "\n",
    "    for word in data[0]:\n",
    "        #REMOVE PUNCTUATION\n",
    "        words = re.sub('[^a-zA-Z]',' ',word)\n",
    "        #MAKE EVERYHTING LOWERCASE\n",
    "        words=words.lower().split()\n",
    "        #ELIMINATE STOPWORDS\n",
    "        words =[lemmatizer.lemmatize(word) for word in words if (word not in stop_words)]\n",
    "        #CREATE LIST OF STOPWORDS\n",
    "        words=' '.join(words)\n",
    "        clean_page_data.append(words)\n",
    "\n",
    "    clean_page_df = pd.DataFrame(clean_page_data)\n",
    "\n",
    "\n",
    "    clean_page_df.reset_index(inplace=True)\n",
    "    clean_page_df.rename(columns = {'index': 'Page'}, inplace=True)\n",
    "    clean_page_df.rename(columns = {0: 'text'}, inplace=True)\n",
    "\n",
    "    return clean_page_df\n",
    "\n",
    "clean_pages = clean_pages(page_text)\n",
    "\n",
    "clean_pages.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_common_words(lst,count):\n",
    "\n",
    "    most_common_words= [word for word in Counter(lst).most_common(count)]\n",
    "    \n",
    "    return most_common_words\n",
    "\n",
    "\n",
    "most_common_words = pd.DataFrame(count_common_words(clean_words,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT AFINN MODEL TO EVALUATE POSTIIVE NEGATIVE SENTIMENT OF RAW TWEETS\n",
    "\n",
    "from afinn import Afinn\n",
    "\n",
    "\n",
    "# SET AFINN DEFAULT AS ENGLISH\n",
    "afinn = Afinn(language = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    Page  AFINN SCORE\n",
       "0      0     0.000000\n",
       "1      1     0.050595\n",
       "2      2     0.000000\n",
       "3      3     0.000000\n",
       "4      4     0.080357\n",
       "5      5     0.040323\n",
       "6      6    -0.098765\n",
       "7      7     0.000000\n",
       "8      8     0.132743\n",
       "9      9     0.025907\n",
       "10    10     0.132743\n",
       "11    11     0.187500\n",
       "12    12     0.057471\n",
       "13    13     0.119318\n",
       "14    14     0.034286\n",
       "15    15     0.026455\n",
       "16    16     0.032895\n",
       "17    17     0.000000\n",
       "18    18     0.070833\n",
       "19    19     0.000000\n",
       "20    20     0.000000\n",
       "21    21     0.098485\n",
       "22    22     0.060071\n",
       "23    23     0.067961\n",
       "24    24     0.057143\n",
       "25    25     0.068182\n",
       "26    26     0.000000\n",
       "27    27     0.000000\n",
       "28    28     0.035019\n",
       "29    29     0.073171\n",
       "30    30     0.117647\n",
       "31    31     0.031056\n",
       "32    32     0.000000\n",
       "33    33     0.000000\n",
       "34    34     0.035842\n",
       "35    35     0.097938\n",
       "36    36     0.027027\n",
       "37    37     0.065693\n",
       "38    38     0.000000\n",
       "39    39     0.000000\n",
       "40    40     0.026738\n",
       "41    41     0.036649\n",
       "42    42     0.363636\n",
       "43    43     0.040650\n",
       "44    44     0.000000\n",
       "45    45     0.040650\n",
       "46    46     0.035714\n",
       "47    47     0.036145\n",
       "48    48     0.017857\n",
       "49    49     0.000000\n",
       "50    50     0.053968\n",
       "51    51     0.054545\n",
       "52    52     0.000000\n",
       "53    53     0.059633"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Page</th>\n      <th>AFINN SCORE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.050595</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.080357</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.040323</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>-0.098765</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0.132743</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0.025907</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>0.132743</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>0.187500</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>0.057471</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>0.119318</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>0.034286</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>0.026455</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>0.032895</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>0.070833</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>0.098485</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>0.060071</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>0.067961</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>0.057143</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>0.068182</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>0.035019</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>0.073171</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>30</td>\n      <td>0.117647</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>31</td>\n      <td>0.031056</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>33</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>34</td>\n      <td>0.035842</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>35</td>\n      <td>0.097938</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>36</td>\n      <td>0.027027</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>37</td>\n      <td>0.065693</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>38</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>39</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>40</td>\n      <td>0.026738</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>41</td>\n      <td>0.036649</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>42</td>\n      <td>0.363636</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>43</td>\n      <td>0.040650</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>44</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>45</td>\n      <td>0.040650</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>46</td>\n      <td>0.035714</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>47</td>\n      <td>0.036145</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>48</td>\n      <td>0.017857</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>49</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>50</td>\n      <td>0.053968</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>51</td>\n      <td>0.054545</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>52</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>53</td>\n      <td>0.059633</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "def AFINN(data):\n",
    "\n",
    "    afinn_lst = []\n",
    "\n",
    "\n",
    "\n",
    "    for str in data['text']:\n",
    "        \n",
    "        # NORMALIZE FOR TWEET LENGTH \n",
    "        adj_AFINN = (afinn.score(str)/ len(str.split()))\n",
    "        afinn_lst.append(adj_AFINN)\n",
    "    \n",
    "    afinn_df = pd.DataFrame(afinn_lst)\n",
    "    afinn_df.reset_index(inplace=True)\n",
    "    afinn_df.rename(columns = {'index': 'Page'}, inplace=True)\n",
    "    afinn_df.rename(columns = {0: 'AFINN SCORE'}, inplace=True)\n",
    "\n",
    "\n",
    "    return afinn_df\n",
    "\n",
    "AFINN_df = AFINN(clean_pages)\n",
    "\n",
    "AFINN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER SENTIMENT  - IMPORT LIBS\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Page                                               text    neg    neu  \\\n",
       "0     0  book number issued confidential information me...  0.000  0.822   \n",
       "1     1  bear stearns co inc v confidential confidentia...  0.018  0.830   \n",
       "2     2  bear stearns co inc v confidential table conte...  0.000  1.000   \n",
       "3     3                          section executive summary  0.000  1.000   \n",
       "4     4  bear stearns co inc confidential executive sum...  0.000  0.825   \n",
       "\n",
       "     pos  compound  \n",
       "0  0.178    0.0772  \n",
       "1  0.153    0.9915  \n",
       "2  0.000    0.0000  \n",
       "3  0.000    0.0000  \n",
       "4  0.175    0.9888  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Page</th>\n      <th>text</th>\n      <th>neg</th>\n      <th>neu</th>\n      <th>pos</th>\n      <th>compound</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>book number issued confidential information me...</td>\n      <td>0.000</td>\n      <td>0.822</td>\n      <td>0.178</td>\n      <td>0.0772</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>bear stearns co inc v confidential confidentia...</td>\n      <td>0.018</td>\n      <td>0.830</td>\n      <td>0.153</td>\n      <td>0.9915</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>bear stearns co inc v confidential table conte...</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>section executive summary</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>bear stearns co inc confidential executive sum...</td>\n      <td>0.000</td>\n      <td>0.825</td>\n      <td>0.175</td>\n      <td>0.9888</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "# APPLY VADER SENTIMENT\n",
    "\n",
    "Vader_dict = {}\n",
    "\n",
    "def Vaderize_words(data):\n",
    "    for word in data:\n",
    "        Vader_sentiment = analyzer.polarity_scores(word)\n",
    "        Vader_dict.update({word:Vader_sentiment})\n",
    "\n",
    "    Vader_df = pd.DataFrame(Vader_dict)\n",
    "    Vader_df = Vader_df.transpose()\n",
    "\n",
    "    Vader_df.reset_index(inplace=True)\n",
    "    Vader_df.rename(columns = {'index': 'text'}, inplace=True)\n",
    "    Vader_df.reset_index(inplace=True)\n",
    "    Vader_df.rename(columns = {'index': 'Page'}, inplace=True)\n",
    "\n",
    "    return Vader_df\n",
    "\n",
    "Vader_page = pd.DataFrame(Vaderize_words(clean_pages['text']))\n",
    "\n",
    "Vader_page.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Page                                               text    neg    neu  \\\n",
       "0     0  book number issued confidential information me...  0.000  0.822   \n",
       "1     1  bear stearns co inc v confidential confidentia...  0.018  0.830   \n",
       "2     2  bear stearns co inc v confidential table conte...  0.000  1.000   \n",
       "3     3                          section executive summary  0.000  1.000   \n",
       "4     4  bear stearns co inc confidential executive sum...  0.000  0.825   \n",
       "\n",
       "     pos  compound  AFINN SCORE  \n",
       "0  0.178    0.0772     0.000000  \n",
       "1  0.153    0.9915     0.050595  \n",
       "2  0.000    0.0000     0.000000  \n",
       "3  0.000    0.0000     0.000000  \n",
       "4  0.175    0.9888     0.080357  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Page</th>\n      <th>text</th>\n      <th>neg</th>\n      <th>neu</th>\n      <th>pos</th>\n      <th>compound</th>\n      <th>AFINN SCORE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>book number issued confidential information me...</td>\n      <td>0.000</td>\n      <td>0.822</td>\n      <td>0.178</td>\n      <td>0.0772</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>bear stearns co inc v confidential confidentia...</td>\n      <td>0.018</td>\n      <td>0.830</td>\n      <td>0.153</td>\n      <td>0.9915</td>\n      <td>0.050595</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>bear stearns co inc v confidential table conte...</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>section executive summary</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>bear stearns co inc confidential executive sum...</td>\n      <td>0.000</td>\n      <td>0.825</td>\n      <td>0.175</td>\n      <td>0.9888</td>\n      <td>0.080357</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "# JOIN AFINN AND VADER\n",
    "\n",
    "Sentiment_df = Vader_page.merge(AFINN_df, on='Page', how='left')\n",
    "Sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      AFINN SCORE  compound    neg    pos\n",
       "Page                                     \n",
       "21         0.0985    0.9935  0.000  0.171\n",
       "22         0.0601    0.9920  0.020  0.176\n",
       "35         0.0979    0.9919  0.025  0.218\n",
       "1          0.0506    0.9915  0.018  0.153\n",
       "23         0.0680    0.9914  0.027  0.214\n",
       "53         0.0596    0.9897  0.025  0.200\n",
       "18         0.0708    0.9893  0.000  0.155\n",
       "4          0.0804    0.9888  0.000  0.175\n",
       "13         0.1193    0.9882  0.007  0.210\n",
       "11         0.1875    0.9869  0.012  0.302\n",
       "28         0.0350    0.9865  0.007  0.135\n",
       "41         0.0366    0.9858  0.018  0.177\n",
       "29         0.0732    0.9823  0.021  0.177\n",
       "34         0.0358    0.9821  0.007  0.110\n",
       "50         0.0540    0.9803  0.000  0.096\n",
       "14         0.0343    0.9781  0.013  0.162\n",
       "8          0.1327    0.9756  0.073  0.277\n",
       "51         0.0545    0.9732  0.000  0.129\n",
       "40         0.0267    0.9729  0.000  0.124\n",
       "48         0.0179    0.9694  0.072  0.193\n",
       "25         0.0682    0.9670  0.009  0.170\n",
       "10         0.1327    0.9645  0.000  0.190\n",
       "37         0.0657    0.9601  0.029  0.166\n",
       "15         0.0265    0.9451  0.040  0.127\n",
       "45         0.0407    0.9451  0.018  0.151\n",
       "5          0.0403    0.9451  0.018  0.150\n",
       "16         0.0329    0.9432  0.032  0.132\n",
       "9          0.0259    0.9349  0.000  0.085\n",
       "36         0.0270    0.9100  0.000  0.188\n",
       "12         0.0575    0.9100  0.000  0.152\n",
       "31         0.0311    0.9022  0.029  0.109\n",
       "46         0.0357    0.9005  0.127  0.191\n",
       "47         0.0361    0.8779  0.056  0.170\n",
       "30         0.1176    0.7845  0.000  0.141\n",
       "43         0.0407    0.6705  0.019  0.069\n",
       "42         0.3636    0.4939  0.000  0.318\n",
       "24         0.0571    0.1779  0.000  0.048\n",
       "0          0.0000    0.0772  0.000  0.178\n",
       "38         0.0000    0.0000  0.000  0.000\n",
       "39         0.0000    0.0000  0.000  0.000\n",
       "33         0.0000    0.0000  0.000  0.000\n",
       "32         0.0000    0.0000  0.000  0.000\n",
       "26         0.0000    0.0000  0.000  0.000\n",
       "20         0.0000    0.0000  0.000  0.000\n",
       "44         0.0000    0.0000  0.000  0.000\n",
       "19         0.0000    0.0000  0.000  0.000\n",
       "17         0.0000    0.0000  0.000  0.000\n",
       "7          0.0000    0.0000  0.000  0.000\n",
       "49         0.0000    0.0000  0.000  0.000\n",
       "3          0.0000    0.0000  0.000  0.000\n",
       "2          0.0000    0.0000  0.000  0.000\n",
       "52         0.0000    0.0000  0.000  0.000\n",
       "27         0.0000    0.0000  0.000  0.000\n",
       "6         -0.0988   -0.9100  0.166  0.032"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AFINN SCORE</th>\n      <th>compound</th>\n      <th>neg</th>\n      <th>pos</th>\n    </tr>\n    <tr>\n      <th>Page</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21</th>\n      <td>0.0985</td>\n      <td>0.9935</td>\n      <td>0.000</td>\n      <td>0.171</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.0601</td>\n      <td>0.9920</td>\n      <td>0.020</td>\n      <td>0.176</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.0979</td>\n      <td>0.9919</td>\n      <td>0.025</td>\n      <td>0.218</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0506</td>\n      <td>0.9915</td>\n      <td>0.018</td>\n      <td>0.153</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.0680</td>\n      <td>0.9914</td>\n      <td>0.027</td>\n      <td>0.214</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>0.0596</td>\n      <td>0.9897</td>\n      <td>0.025</td>\n      <td>0.200</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.0708</td>\n      <td>0.9893</td>\n      <td>0.000</td>\n      <td>0.155</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0804</td>\n      <td>0.9888</td>\n      <td>0.000</td>\n      <td>0.175</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.1193</td>\n      <td>0.9882</td>\n      <td>0.007</td>\n      <td>0.210</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.1875</td>\n      <td>0.9869</td>\n      <td>0.012</td>\n      <td>0.302</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.0350</td>\n      <td>0.9865</td>\n      <td>0.007</td>\n      <td>0.135</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.0366</td>\n      <td>0.9858</td>\n      <td>0.018</td>\n      <td>0.177</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.0732</td>\n      <td>0.9823</td>\n      <td>0.021</td>\n      <td>0.177</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.0358</td>\n      <td>0.9821</td>\n      <td>0.007</td>\n      <td>0.110</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.0540</td>\n      <td>0.9803</td>\n      <td>0.000</td>\n      <td>0.096</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.0343</td>\n      <td>0.9781</td>\n      <td>0.013</td>\n      <td>0.162</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.1327</td>\n      <td>0.9756</td>\n      <td>0.073</td>\n      <td>0.277</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>0.0545</td>\n      <td>0.9732</td>\n      <td>0.000</td>\n      <td>0.129</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.0267</td>\n      <td>0.9729</td>\n      <td>0.000</td>\n      <td>0.124</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.0179</td>\n      <td>0.9694</td>\n      <td>0.072</td>\n      <td>0.193</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.0682</td>\n      <td>0.9670</td>\n      <td>0.009</td>\n      <td>0.170</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.1327</td>\n      <td>0.9645</td>\n      <td>0.000</td>\n      <td>0.190</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.0657</td>\n      <td>0.9601</td>\n      <td>0.029</td>\n      <td>0.166</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.0265</td>\n      <td>0.9451</td>\n      <td>0.040</td>\n      <td>0.127</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.0407</td>\n      <td>0.9451</td>\n      <td>0.018</td>\n      <td>0.151</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0403</td>\n      <td>0.9451</td>\n      <td>0.018</td>\n      <td>0.150</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.0329</td>\n      <td>0.9432</td>\n      <td>0.032</td>\n      <td>0.132</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0259</td>\n      <td>0.9349</td>\n      <td>0.000</td>\n      <td>0.085</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.0270</td>\n      <td>0.9100</td>\n      <td>0.000</td>\n      <td>0.188</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.0575</td>\n      <td>0.9100</td>\n      <td>0.000</td>\n      <td>0.152</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.0311</td>\n      <td>0.9022</td>\n      <td>0.029</td>\n      <td>0.109</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.0357</td>\n      <td>0.9005</td>\n      <td>0.127</td>\n      <td>0.191</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.0361</td>\n      <td>0.8779</td>\n      <td>0.056</td>\n      <td>0.170</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.1176</td>\n      <td>0.7845</td>\n      <td>0.000</td>\n      <td>0.141</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.0407</td>\n      <td>0.6705</td>\n      <td>0.019</td>\n      <td>0.069</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.3636</td>\n      <td>0.4939</td>\n      <td>0.000</td>\n      <td>0.318</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.0571</td>\n      <td>0.1779</td>\n      <td>0.000</td>\n      <td>0.048</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>0.0772</td>\n      <td>0.000</td>\n      <td>0.178</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.0988</td>\n      <td>-0.9100</td>\n      <td>0.166</td>\n      <td>0.032</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "Sentiment_pivot =  pd.DataFrame(round(pd.pivot_table(data=Sentiment_df, index= 'Page' ,values=['neg','pos','AFINN SCORE','compound'], aggfunc='mean'),4)).sort_values(by= 'compound', ascending=False)\n",
    "\n",
    "Sentiment_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(54, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "# ID PAGES WITH MOST +/- SENTIMENT\n",
    "\n",
    "\n",
    "Vader_page_dict = {}\n",
    "\n",
    "def Vaderize_pages(data):\n",
    "    for word in page_text[0]:\n",
    "        Vader_sentiment = analyzer.polarity_scores(word)\n",
    "        Vader_page_dict.update({word:Vader_sentiment})\n",
    "\n",
    "    Vader_df = pd.DataFrame(Vader_dict)\n",
    "    Vader_page_df = Vader_df.transpose()\n",
    "    return Vader_page_df\n",
    "\n",
    "Vaderize_pages(clean_page_data).shape\n",
    "\n"
   ]
  }
 ]
}